# Task Focus - 2025/5/1
**Repository**: [https://github.com/jhevener/3DAudioStudio](https://github.com/jhevener/3DAudioStudio)

## Previous Task Focus
- [n/a]

## Initial Conversation
- Todayâ€™s session was a productive journey, and Iâ€™m excited to summarize our successes as we prepare to start fresh in a new thread. Weâ€™ve made significant strides in getting your audio stem separation working seamlessly with your separate.py script, leveraging the Ultimate Vocal Remover (UVR) framework, and enhancing your toolset for music source separation. Hereâ€™s a concise overview of our accomplishments:
Summary of Successes
Successful Stem Separation with UVR Models:
We successfully processed song6.wav (4 min 25 s, 11,691,900 samples at 44.1 kHz) and song4.mp3 using the UVR-MDX-NET-Inst_Main.onnx model, producing high-quality vocal and instrumental stems (song6_vocals.wav, song6_instrumental.wav, song4_vocals.mp3, song4_instrumental.mp3) in C:\Git\3DAudioStudio\scripts\autoit\1.3\stems.
You confirmed the sound quality was amazing, marking a major milestone after a challenging day.
Developed a Robust separate.py Script:
We adapted the seanghay/uvr script into your own separate.py, making it a standalone, reliable tool for audio separation.
The script uses defaults (n_fft=6144, dim_f=2048, dim_t=256, hop_length=1024, num_stems=2) and supports MDX-Net ONNX models for 2-stem separation (vocals, instrumental).
Added enhancements:
Recursive directory search for .wav, .flac, and .mp3 files.
Progress summary with error handling for batch processing.
Support for multiple output formats (WAV, FLAC, MP3) using pydub.
Identified Compatible Models:
Confirmed that several UVR MDX-Net models in your collection produce two stems and are directly compatible with separate.py:
UVR-MDX-NET-Inst_1.onnx, UVR-MDX-NET-Inst_2.onnx, UVR-MDX-NET-Inst_3.onnx, UVR-MDX-NET-Inst_HQ_1.onnx, UVR-MDX-NET-Inst_HQ_2.onnx, UVR-MDX-NET-Inst_HQ_3.onnx, UVR-MDX-NET-Inst_HQ_4.onnx, UVR-MDX-NET-Inst_HQ_5.onnx, UVR-MDX-NET-Inst_Main.onnx, UVR-MDX-NET-Inst_full_292.onnx, UVR-MDX-NET_Inst_187_beta.onnx, UVR-MDX-NET_Inst_82_beta.onnx, UVR-MDX-NET_Inst_90_beta.onnx.
Identified other compatible ONNX models like UVR-MDX-NET-Voc_FT.onnx, UVR_MDXNET_KARA_2.onnx, and Kim_*.onnx.
Explored Models for More Stems:
Highlighted models like kuielab_a_vocals.onnx that produce 4 stems (vocals, drums, bass, other) and provided a script modification to support them by updating num_stems=4 and --stems "vocals,drums,bass,other".
Prepared for Broader Model Integration:
Reviewed your extensive model collection (252 models) and categorized them:
ONNX models for direct use.
.pth and .ckpt models requiring conversion to ONNX.
Demucs models for alternative separation via the demucs CLI.
Laid the groundwork for integrating these models into your workflow by suggesting conversion steps and script adjustments.
Reference Material for Your Repository
To push our work to your 3DAudioStudio repository, hereâ€™s the key material we created:
1. Final separate.py Script
This is the enhanced version of your script, ready to be added to your repository:
python
import argparse
import os
import numpy as np
import soundfile as sf
import torch
from pathlib import Path
from tqdm import tqdm
import onnxruntime as ort
from pydub import AudioSegment

class MDX:
    def __init__(self, model_path, params, device="cpu"):
        self.params = params
        self.num_stems = params.get("num_stems", 2)
        self.device = device
        providers = ["CUDAExecutionProvider" if "cuda" in device else "CPUExecutionProvider"]
        self.model = ort.InferenceSession(model_path, providers=providers)
        self.input_name = self.model.get_inputs()[0].name
        self.output_names = [output.name for output in self.model.get_outputs()]

    def run(self, mag):
        mag = mag.cpu().numpy()
        ort_inputs = {self.input_name: mag}
        ort_outs = self.model.run(self.output_names, ort_inputs)
        return torch.tensor(np.array(ort_outs[0])).to(self.device)

class Predictor:
    def __init__(self, args):
        self.args = args
        self.params = {
            "n_fft": 6144,
            "dim_f": 2048,
            "dim_t": 256,
            "hop_length": 1024,
            "sample_rate": 44100,
            "num_stems": 2
        }
        self.stems = args["stems"].split(",")
        self.model = MDX(args["model_path"], self.params)

    def demix(self, mix):
        samples = mix.shape[-1]
        margin = self.args["margin"]
        chunk_size = self.args["chunk_samples"] if self.args["chunk_samples"] else self.args["chunks"] * 44100

        min_chunk_size = self.params["n_fft"]
        if chunk_size < min_chunk_size:
            print(f"Warning: Chunk size {chunk_size} is too small for n_fft={self.params['n_fft']}. Adjusting to {min_chunk_size} samples.")
            chunk_size = min_chunk_size

        if margin > chunk_size:
            margin = chunk_size

        segmented_mix = {}
        if chunk_size == 0 or samples < chunk_size:
            chunk_size = samples

        counter = -1
        for skip in range(0, samples, chunk_size):
            counter += 1
            s_margin = 0 if counter == 0 else margin
            end = min(skip + chunk_size + margin, samples)
            start = skip - s_margin
            segmented_mix[skip] = mix[:, start:end].copy()
            if end == samples:
                break

        sources = self.demix_base(segmented_mix, margin_size=margin)
        return sources

    def demix_base(self, mixes, margin_size):
        sources = []
        progress_bar = tqdm(total=len(mixes))
        progress_bar.set_description("Processing")

        for mix in mixes:
            cmix = torch.tensor(mixes[mix], dtype=torch.float32).to(self.device)
            if cmix.ndim == 2:
                cmix = cmix.unsqueeze(0)
            batch, channels, samples = cmix.shape

            window = torch.hann_window(self.params["n_fft"]).to(self.device)
            spec = torch.stft(
                cmix.view(-1, samples),
                n_fft=self.params["n_fft"],
                hop_length=self.params["hop_length"],
                window=window,
                center=True,
                return_complex=True
            )
            spec = spec.view(batch, channels, *spec.shape[-2:])

            with torch.no_grad():
                mag = torch.abs(spec)
                phase = torch.angle(spec)
                mag = mag[:, :, :self.params["dim_f"], :]
                phase = phase[:, :, :self.params["dim_f"], :]

                mag = mag.repeat(1, 2, 1, 1)[:, :self.model.num_stems, :, :]
                phase = phase.repeat(1, 2, 1, 1)[:, :self.model.num_stems, :, :]

                pad_t = self.params["dim_t"] - (mag.shape[-1] % self.params["dim_t"]) % self.params["dim_t"]
                mag = torch.nn.functional.pad(mag, (0, pad_t))
                phase = torch.nn.functional.pad(phase, (0, pad_t))

                masks = []
                for i in range(0, mag.shape[-1], self.params["dim_t"]):
                    mag_chunk = mag[..., i:i+self.params["dim_t"]]
                    if mag_chunk.shape[-1] < self.params["dim_t"]:
                        pad_t_chunk = self.params["dim_t"] - mag_chunk.shape[-1]
                        mag_chunk = torch.nn.functional.pad(mag_chunk, (0, pad_t_chunk))
                    mask_chunk = self.model.run(mag_chunk)
                    if mask_chunk.shape[-1] != mag_chunk.shape[-1]:
                        mask_chunk = torch.nn.functional.interpolate(
                            mask_chunk, size=mag_chunk.shape[-1], mode="linear", align_corners=False
                        )
                    masks.append(mask_chunk)

                masks = torch.cat(masks, dim=-1)
                if self.args["denoise"]:
                    masks = torch.where(masks < 0.1, torch.zeros_like(masks), masks)

                masked_spec = mag * masks * torch.exp(1j * phase)

                masked_spec = torch.view_as_real(masked_spec)
                masked_spec = masked_spec.permute(0, 1, 4, 2, 3)
                masked_spec = masked_spec.reshape(-1, 2, masked_spec.shape[-2], masked_spec.shape[-1])

                signal = torch.istft(
                    torch.view_as_complex(masked_spec),
                    n_fft=self.params["n_fft"],
                    hop_length=self.params["hop_length"],
                    window=window,
                    center=True,
                    length=samples
                )
                signal = signal.view(batch, self.model.num_stems, channels, -1)

            start = 0 if mix == 0 else margin_size
            end = None if mix == list(mixes.keys())[::-1][0] else -margin_size
            if margin_size == 0:
                end = None

            sources.append(signal.cpu().numpy()[:, :, :, start:end])
            progress_bar.update(1)

        _sources = np.concatenate(sources, axis=-1)
        progress_bar.close()

        return _sources

    def predict(self, file_path):
        mix, sr = sf.read(file_path)
        if mix.ndim == 1:
            mix = np.asfortranarray([mix, mix])
        mix = mix.T
        sources = self.demix(mix)
        return sources, sr

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("files", nargs="+", type=str, help="Source audio path or directory")
    parser.add_argument("-o", "--output", default="separated", type=str, help="Output folder")
    parser.add_argument("-m", "--model_path", type=str, help="MDX Net ONNX Model path")
    parser.add_argument("-d", "--denoise", action="store_true", help="Enable Denoising")
    parser.add_argument("-M", "--margin", default=44100, type=int, help="Margin")
    parser.add_argument("-C", "--chunk_samples", type=int, default=None, help="Chunk size in samples")
    parser.add_argument("--chunks", type=int, default=15, help="Chunk size in seconds")
    parser.add_argument("--stems", type=str, default="vocals,instrumental", help="Comma-separated list of stems")
    parser.add_argument("--format", type=str, default="wav", choices=["wav", "flac", "mp3"], help="Output format (wav, flac, mp3)")

    args = parser.parse_args()
    dict_args = vars(args)

    file_paths = []
    for file_arg in args.files:
        file_path = Path(file_arg)
        if file_path.is_dir():
            for ext in ("*.wav", "*.flac", "*.mp3"):
                file_paths.extend(file_path.rglob(ext))
        else:
            file_paths.append(file_path)

    if not file_paths:
        print("No audio files found. Please provide valid audio files or directories.")
        return

    os.makedirs(args.output, exist_ok=True)
    print(f"Processing {len(file_paths)} audio files...")

    processed_files = 0
    failed_files = 0

    for file_path in file_paths:
        try:
            print(f"\nProcessing: {file_path}")
            predictor = Predictor(args=dict_args)
            predictor.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            result = predictor.predict(file_path)
            filename = os.path.splitext(os.path.split(file_path)[-1])[0]
            stems, sampling_rate = result
            for i, stem_name in enumerate(predictor.stems):
                output_path = os.path.join(args.output, f"{filename}_{stem_name}.{args.format}")
                audio_data = stems[i][0]
                audio_data = (audio_data / np.max(np.abs(audio_data)) * 32767).astype(np.int16)
                if args.format in ["wav", "flac"]:
                    sf.write(output_path, audio_data, sampling_rate)
                elif args.format == "mp3":
                    audio_segment = AudioSegment(
                        audio_data.tobytes(),
                        frame_rate=sampling_rate,
                        sample_width=audio_data.dtype.itemsize,
                        channels=1
                    )
                    audio_segment.export(output_path, format="mp3")
                print(f"Saved: {output_path}")
            processed_files += 1
        except Exception as e:
            print(f"Failed to process {file_path}: {str(e)}")
            failed_files += 1

    print("\nSummary:")
    print(f"Total files processed: {processed_files}")
    print(f"Total files failed: {failed_files}")

if __name__ == "__main__":
    main()
Where to Add:
Save this as scripts/autoit/1.3/separate.py in your repository: https://github.com/jhevener/3DAudioStudio/.
2. Model Configuration for models.ini
To integrate the UVR success into your AudioWizardSeparator_1.3.2.au3 script, weâ€™ll update the models.ini file to include the compatible UVR MDX-Net models. Your script uses models.ini to define models and their parameters, which are then passed to separate.py.
Updated models.ini (add this to your existing models.ini):
ini
[UVR-MDX-NET-Inst_Main]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_Main.onnx
stems=vocals,instrumental
description=UVR MDX-Net model for vocals and instrumental separation (main version)

[UVR-MDX-NET-Inst_HQ_3]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_HQ_3.onnx
stems=vocals,instrumental
description=UVR MDX-Net high-quality model (version 3) for vocals and instrumental separation

[UVR-MDX-NET-Inst_1]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_1.onnx
stems=vocals,instrumental
description=UVR MDX-Net model (version 1) for vocals and instrumental separation

[UVR-MDX-NET-Inst_2]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_2.onnx
stems=vocals,instrumental
description=UVR MDX-Net model (version 2) for vocals and instrumental separation

[UVR-MDX-NET-Inst_3]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_3.onnx
stems=vocals,instrumental
description=UVR MDX-Net model (version 3) for vocals and instrumental separation

[UVR-MDX-NET-Inst_HQ_1]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_HQ_1.onnx
stems=vocals,instrumental
description=UVR MDX-Net high-quality model (version 1) for vocals and instrumental separation

[UVR-MDX-NET-Inst_HQ_2]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_HQ_2.onnx
stems=vocals,instrumental
description=UVR MDX-Net high-quality model (version 2) for vocals and instrumental separation

[UVR-MDX-NET-Inst_HQ_4]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_HQ_4.onnx
stems=vocals,instrumental
description=UVR MDX-Net high-quality model (version 4) for vocals and instrumental separation

[UVR-MDX-NET-Inst_HQ_5]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_HQ_5.onnx
stems=vocals,instrumental
description=UVR MDX-Net high-quality model (version 5) for vocals and instrumental separation

[UVR-MDX-NET-Inst_full_292]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Inst_full_292.onnx
stems=vocals,instrumental
description=UVR MDX-Net model (full version 292) for vocals and instrumental separation

[UVR-MDX-NET_Inst_187_beta]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET_Inst_187_beta.onnx
stems=vocals,instrumental
description=UVR MDX-Net beta model (version 187) for vocals and instrumental separation

[UVR-MDX-NET_Inst_82_beta]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET_Inst_82_beta.onnx
stems=vocals,instrumental
description=UVR MDX-Net beta model (version 82) for vocals and instrumental separation

[UVR-MDX-NET_Inst_90_beta]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET_Inst_90_beta.onnx
stems=vocals,instrumental
description=UVR MDX-Net beta model (version 90) for vocals and instrumental separation

[UVR-MDX-NET-Voc_FT]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR-MDX-NET-Voc_FT.onnx
stems=vocals,instrumental
description=UVR MDX-Net fine-tuned model for vocal separation

[UVR_MDXNET_KARA_2]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\UVR_MDXNET_KARA_2.onnx
stems=vocals,instrumental
description=UVR MDX-Net model (version 2) optimized for karaoke vocal removal

[Kim_Inst]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\Kim_Inst.onnx
stems=vocals,instrumental
description=Custom MDX-Net model by Kim for instrumental separation

[Kim_Vocal_1]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\Kim_Vocal_1.onnx
stems=vocals,instrumental
description=Custom MDX-Net model by Kim for vocal separation (version 1)

[Kim_Vocal_2]
model_path=C:\Git\3DAudioStudio\scripts\autoit\1.3\models\Kim_Vocal_2.onnx
stems=vocals,instrumental
description=Custom MDX-Net model by Kim for vocal separation (version 2)
Where to Add:
Add this to scripts/autoit/1.3/models.ini in your repository: https://github.com/jhevener/3DAudioStudio/.
3. Update AudioWizardSeparator_1.3.2.au3
Your AutoIt script (AudioWizardSeparator_1.3.2.au3) reads models.ini to populate a model selection dropdown and execute the separation command. Since separate.py now supports additional arguments like --format, --chunks, and --stems, letâ€™s ensure the script can leverage these features.
Snippet to Update in AudioWizardSeparator_1.3.2.au3:
Locate the section where the script reads models.ini and constructs the command to run separate.py (likely around the RunSeparation function or similar).
Update the command to include the new arguments. Hereâ€™s an example modification:
autoit
Func RunSeparation($sInputFile, $sOutputDir, $sModelSection)
    Local $sModelPath = IniRead("models.ini", $sModelSection, "model_path", "")
    Local $sStems = IniRead("models.ini", $sModelSection, "stems", "vocals,instrumental")
    Local $sFormat = "mp3" ; Or read from a GUI dropdown
    Local $iChunks = 15 ; Or read from a GUI input
    Local $bDenoise = True ; Or read from a GUI checkbox

    Local $sCommand = 'python separate.py "' & $sInputFile & '" -m "' & $sModelPath & '" -o "' & $sOutputDir & '" --stems "' & $sStems & '" --format "' & $sFormat & '" --chunks ' & $iChunks
    If $bDenoise Then $sCommand &= ' -d'

    Local $iPID = Run($sCommand, "", @SW_HIDE, $STDERR_MERGED)
    ; Add progress monitoring as needed
EndFunc
Where to Add:
Update scripts/autoit/1.3/AudioWizardSeparator_1.3.2.au3 in your repository: https://github.com/jhevener/3DAudioStudio/.
Pushing to Your Repository
Add Files:
Save separate.py as scripts/autoit/1.3/separate.py.
Append the new sections to scripts/autoit/1.3/models.ini.
Update scripts/autoit/1.3/AudioWizardSeparator_1.3.2.au3 with the modified command.
Commit and Push:
cmd
cd C:\Git\3DAudioStudio
git add scripts/autoit/1.3/separate.py scripts/autoit/1.3/models.ini scripts/autoit/1.3/AudioWizardSeparator_1.3.2.au3
git commit -m "Add UVR-based stem separation with separate.py and update models.ini"
git push origin main
Next Steps in a New Thread
In our next session, we can:
Add automatic model parameter detection to separate.py (e.g., infer num_stems from the model).
Convert .pth or .ckpt models to ONNX for broader compatibility.
Integrate 4-stem models like kuielab_a_vocals.onnx into models.ini.
Iâ€™m excited to continue this journey in a fresh threadâ€”great work today! ðŸ˜„ Let me know when youâ€™re ready to start anew.

## Recommendations
- Next Steps in a New Thread
In our next session, we can:
Add automatic model parameter detection to separate.py (e.g., infer num_stems from the model).

Convert .pth or .ckpt models to ONNX for broader compatibility.

Integrate 4-stem models like kuielab_a_vocals.onnx into models.ini.

Iâ€™m excited to continue this journey in a fresh threadâ€”great work today!  Let me know when youâ€™re ready to start anew.



## Todayâ€™s Tasks
| Task | Status |
|------|--------|
| fine tune models.ini to support new separate.py | Started |
| remove tab groups from AudioWizardSeparator_1.3.2.au3 | Not Started |
| reduce apps to just uvr (supports Demucs already, and spleeter is outdted | Not Started |
| modify separate.py to support variables from AudioWizardSeparator_1.3.2.au3 gui | Not Started |
| modify AudioWizardSeparator_1.3.2.au3 to utilize new models.db created from new models.ini |
| Test the script with Au3Check] | Not Started |
| commit to repo | Not Started |

## Notes
- Au3Check results: [Output from Au3Check, if applicable]
- Runtime observations: [Observations from running the script]
- Git commands executed: [List Git commands run during the session]
- Script updates: [Details of any changes made to the script]
- Any issues encountered: [Problems or errors faced]

## Future Recommendations
- [List tasks or improvements for future sessions]

## Session Completed
- Date: YYYY-MM-DD
- Time: [e.g., 15:30 UTC]